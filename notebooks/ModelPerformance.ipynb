{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Performance\n",
    "\n",
    "Get ready to put on your data science and ML engineering hats! In the next few lessons, you are going to learn how to evaluate the performance of ML models. Recall that after you train an ML model on past data, you can use that model to make predictions on new or previously unseen data. But how do you know if that model is useful?  In ML, when you hear the phrase \"model performance\", I want you to think about evaluating the quality of model predictions, commonly referred to as its **forecast skill** or **prediction skill**. This first lesson will focus on evaluating predictive modeling in the context of supervised learning, including both classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification Accuracy and Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Classification is about predicting a label, typically a discrete value. For example, an image of an animal may be classified as being a picture of a \"cat\" or \"dog\". There are many ways to measure the prediction skill of a classification model, but **accuracy** and **error rate** are the de facto standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accuracy\n",
    "Accuracy is the ratio of the correct predictions to the total number of predictions made.\n",
    "* Accuracy = Correct Predictions / Total Predictions\n",
    "\n",
    "90% and above for the accuracy of a predictive model is considered to be good, and it is common practice to aim for that level. \n",
    "\n",
    "### Error Rate\n",
    "You can also summarize model performance in terms of the error rate.\n",
    "* Error Rate = Incorrect Predictions / Total Predictions\n",
    "\n",
    "Accuracy and error rates are complements of each other and therefore you can calculate one from the other as follows:\n",
    "* Accuracy = 1 - Error Rate\n",
    "* Error Rate = 1 - Accuracy\n",
    "\n",
    "Consider a classifier that labels pictures as either cats or dogs and that, when tested on 12 pictures (8 cats and 4 dogs), produces the following results:\n",
    "* 9 Correct Predictions   = (9/12) = 0.75 \n",
    "* 3 Incorrect Predictions = (3/12) = 0.25\n",
    "\n",
    "Knowing that the classifier has an accuracy of 0.75 or 75%, does not provide any insight into where the classifier is not performing well. \n",
    "\n",
    "Is it more mistaking cats for dogs, or dogs for cats? or is it about the same? \n",
    "\n",
    "This is where a **confusion matrix** may prove useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Confusion Matrix\n",
    "A confusion matrix, or table of confusion, allows you to easily visualize classification performance and see where the model may be confusing two or more classes. \n",
    "\n",
    "<img src=\"img/confusion_matrix.png\" width=\"200\">\n",
    "\n",
    "In this confusion matrix, of the 8 cat pictures, the model predicted that 2 were dogs, and of the 4 dog pictures, it predicted that 1 was a cat. All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they are represented by values outside the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification Metrics in Scikit-learn: A First Look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Scikit-learn is a free ML library for the Python programming language. It has 3 different programming interfaces for evaluating the quality of a modelâ€™s predictions:\n",
    "\n",
    "* Estimator Score Method\n",
    "* Scoring Parameter\n",
    "* Metrics Functions\n",
    "\n",
    "In this lesson, you'll get hands-on experience using the scikit-learn metrics functions to measure prediction skill in the context of the cat or dog classification example. \n",
    "\n",
    "First start by importing the scikit-learn metrics module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Assume that the actual and predicted values from the example are defined as follows, where cats belong to the class 0 and dogs belong to the class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_values = [0,0,0,0,0,0,0,0,1,1,1,1]\n",
    "predictions =   [1,1,0,0,0,0,0,0,1,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you can use the metrics functions to calculate the accuracy, error rate, and print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.0 % \n",
      "Error Rate: 25.0 % \n",
      "Confusion Matrix:\n",
      "[[6 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {metrics.accuracy_score(actual_values, predictions) * 100} % ')\n",
    "\n",
    "print(f'Error Rate: {(1 - metrics.accuracy_score(actual_values, predictions)) * 100} % ')\n",
    "\n",
    "print(f'Confusion Matrix:')\n",
    "\n",
    "print(metrics.confusion_matrix(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Precision, Recall, and F-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a performance measure, classification accuracy has its limitations. One example where accuracy may be an inadequate performance measure is in the presence of class imbalance. For example, imagine a situation where a dataset of cat and dog images contains a large number of cat examples (majority class) and a small number of dog examples (minority class). On such a dataset, even unskillful models model may achieve high accuracy if the large number of examples from the majority class overwhelms those in the minority class.  \n",
    "\n",
    "An alternative to using classification accuracy is to use **precision** and **recall** metrics. \n",
    "\n",
    "However, prior to getting into precision and recall, it is important to dive deeper into the confusion matrix as it provides insight into both the performance of the model and the types of errors being made.\n",
    "\n",
    "### Confusion Matrix Reloaded\n",
    "\n",
    "The results summary displayed in the confusion matrix consists of true predictions and false predictions.\n",
    "\n",
    "<img src=\"img/confusion_matrix_reloaded.png\" width=\"200\">\n",
    "\n",
    "True Predictions: \n",
    "  * TP: True Positives. Model predicted Yes, and actual value is Yes.\n",
    "  * TN: True Negatives. Model predicted No, and actual value is no.\n",
    "  \n",
    "False Predictions: \n",
    "  * FP: False Positives. Model predicted Yes, but actual value is No.\n",
    "  * FN: False Negatives. Model predicted No, but actual value is Yes.\n",
    "\n",
    "\n",
    "Precision and recall are defined using the terms in the \"reloaded\" confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "\n",
    "### Recall\n",
    "\n",
    "\n",
    "### F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80         8\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.73      0.75      0.73        12\n",
      "weighted avg       0.77      0.75      0.76        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision Score is: {metrics.precision_score(actual_values, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Recall Score is: {metrics.recall_score(actual_values, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision - Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/penguins_size.csv')\n",
    "\n",
    "data = data.dropna()\n",
    "data = data.drop(['sex', 'island', 'flipper_length_mm', 'body_mass_g'], axis=1)\n",
    "data = data[data['species'] != 'Chinstrap']\n",
    "\n",
    "X = data.drop(['species'], axis=1).values\n",
    "\n",
    "y = data['species']\n",
    "spicies = {'Adelie': -1, 'Gentoo': 1}\n",
    "y = [spicies[item] for item in y]\n",
    "y = np.array(y) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=1)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_preditions = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = metrics.plot_precision_recall_curve(dt_model, X_test, y_test, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('F1 Score:', metrics.f1_score(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/penguins_size.csv')\n",
    "\n",
    "data = data.dropna()\n",
    "data = data.drop(['sex', 'island', 'flipper_length_mm', 'body_mass_g'], axis=1)\n",
    "data = data[data['species'] != 'Chinstrap']\n",
    "\n",
    "X = data.drop(['species'], axis=1).values\n",
    "\n",
    "y = data['species']\n",
    "spicies = {'Adelie': -1, 'Gentoo': 1}\n",
    "y = [spicies[item] for item in y]\n",
    "y = np.array(y) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=1)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_preditions = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_roc_curve(dt_model, X_test, y_test, color = 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('AUC-ROC:', metrics.roc_auc_score(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGLOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('LOGLOSS:', metrics.log_loss(actual_values, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
