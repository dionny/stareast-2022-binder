{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Performance: Regression\n",
    "\n",
    "Whereas classification is about predicting a discrete label such as \"cat\" or \"dog\", regression is about predicting a quantity, typically continuous values like amounts, sizes, or prices. For example, consider that a house may be predicted to sell for a specific dollar value. But how do you calculate the accuracy for a regression model? The answer is simple: you don't! Accuracy is a measure of classification, not regression. If you are predicting a numerical value such as the sale price of a house, you don't necessarily want to know if the model predicted the value exactly. Instead, you care more about how close the prediction was to the expected value.  A way to describe the numerical difference between the actual and expected values is **distance** or **error**. This lesson introduces various error metrics that you can use to report the prediction skill of a regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"img/mse.png\" width=\"200\">\n",
    "\n",
    "There are four error metrics that are commonly used for evaluating and reporting on the quality of a regression predictions:\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "* Finds the average squared distance (error) between the predicted and actual values. \n",
    "* Tells you how close a regression line is to a set of points by taking the distances from the points to the line and squaring them.\n",
    "* Squaring removes any (-) negative signs and magnifies large errors. \n",
    "* The lower the MSE, the better the prediction skill.\n",
    "* Formula: \n",
    "  - **MSE** = 1 / N * sum for i to N (y_i – yhat_i)^2\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"img/rmse.png\" width=\"200\">\n",
    "\n",
    "* Variation of the MSE metric which shows what is the average **deviation** in predictions from actual values.\n",
    "* Follows an assumption that error is unbiased and follows a normal distribution.\n",
    "* Just like MSE, RMSE is a non-negative value and the lower the RMSE, the better the prediction skill.\n",
    "* RMSE punishes large errors and is the best metric for large numbers (actual value or prediction). \n",
    "* It is affected by outliers so make sure that you remove them from the dataset beforehand.\n",
    "* Formula: \n",
    "  - **RMSE** = sqrt(1 / N * sum for i to N (y_i – yhat_i)^2)\n",
    "\n",
    "\n",
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"img/mae.png\" width=\"270\">\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "* Computes the average of the absolute error values by forcing the difference between predicted and actual values to be positive.\n",
    "* Unlike the MSE and RMSE that punish larger errors more than smaller errors, the changes in MAE are linear and therefore more intuitive.\n",
    "* MAE gives you information on the magnitude of the error, but no idea of the direction, i.e., Is the model over or under estimating?\n",
    "* Like the others, an error value of 0.0 would be ideal, meaning that all predictions matched the expected values exactly.\n",
    "* Formula: \n",
    "  - **MAE** = 1 / N * sum for i to N abs(y_i – yhat_i)\n",
    "\n",
    "\n",
    "\n",
    "#### R-Squared (R<sup>2</sup>)\n",
    "\n",
    "* Also referred to as the **coefficient of determination**.\n",
    "* Provides an indication of the goodness of fit of a set of predictions to the actual values.\n",
    "* Yields a value between 0 and 1 for no-fit and perfect fit respectively.\n",
    "* Formula: \n",
    "  - **R<sup>2</sup>** = 1 - Unexplained Variance / Total Variance\n",
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"img/rsquared.png\" width=\"500\">\n",
    "\n",
    ">**Calculation**: The actual calculation of R<sup>2</sup> requires several steps, including taking data points (observations) of dependent and independent variables, and finding the line of best fit from a regression model. From there you would calculate predicted values, subtract actual values and, square the results. This yields a list of errors squared, which is then summed and equals the **unexplained variance**.\n",
    "> To calculate the **total variance**, you would subtract the average actual value from each of the actual values, square the results and sum them. From there, divide the first sum of errors (explained variance) by the second sum (total variance), subtract the result from one, and you now have the R-Squared measure.\n",
    "\n",
    ">**Meaning**: R<sup>2</sup> gives you an idea of how many data points fall within the results of the line formed by the regression equation. The higher the coefficient, the higher percentage of points the line passes through when the data points and line are plotted. If the coefficient is 0.80, then 80% of the points should fall within the regression line. Values of 1 or 0 would indicate the regression line represents all or none of the data, respectively. A higher coefficient is an indicator of a better goodness of fit for the observations\n",
    "\n",
    ">**Usefulness**: The usefulness of R<sup>2</sup> is its ability to find the likelihood of future events falling within the predicted outcomes. The idea is that if more samples are added, the coefficient would show the probability of a new point falling on the line. Even if there is a strong connection between the two variables, determination does not prove causality. For example, a study on birthdays may show a large number of birthdays happen within a time frame of one or two months. This does not mean that the passage of time or the change of seasons causes pregnancy.\n",
    "\n",
    "#### Need Help? \n",
    "* If these metrics seem complicated, don't worry... \n",
    "* The good news is that the computing these metrics in Python with open-source libraries is easy\n",
    "* All you have to worry about is knowing:\n",
    "  - The context(s) in which each metric is suitable.\n",
    "  - Any influencing factors or limitations that may impact your evaluation or threaten its validity.\n",
    "  - How to invoke the appropriate metrics functions from your code and interpret the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-On with Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "#Load Prediction Results\n",
    "actual_values = [9, -3.3, 6, 11]\n",
    "predictions =   [8.5, -2.9, 6, 9.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "print (f'MSE:  {metrics.mean_squared_error(actual_values, predictions)}')\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "def rmse(actual_values, predictions):\n",
    "    actual_values = np.asarray(actual_values)\n",
    "    predictions = np.asarray(predictions)\n",
    "    return np.sqrt(((predictions - actual_values) ** 2).mean())\n",
    "print(f'RMSE: {rmse(actual_values, predictions)}')\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "print (f'MAE:  {metrics.mean_absolute_error(actual_values, predictions)}')\n",
    "\n",
    "# Calculate R-Squared\n",
    "print (f'R^2:  {metrics.r2_score(actual_values, predictions)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
